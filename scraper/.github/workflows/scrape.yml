name: Scrape news (daily)

on:
  workflow_dispatch:
    inputs:
      max_per_site:
        description: "Máximo de artículos por sitio"
        required: false
        default: "20"
  schedule:
    - cron: "17 1 * * *"   # 01:17 UTC cada día

permissions:
  contents: write

concurrency:
  group: scrape
  cancel-in-progress: true

jobs:
  scrape:
    runs-on: ubuntu-latest
    defaults:
      run:
        working-directory: scraper

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: "pip"
          cache-dependency-path: scraper/requirements.txt

      - name: Install deps
        run: pip install -r requirements.txt

      - name: Run scraper
        run: |
          MAX=${{ github.event.inputs.max_per_site || '20' }}
          python scraper.py --config sites.yaml --out data/scraped.csv --max-per-site $MAX --with-text

      # Sincroniza con main antes de commitear para evitar non-fast-forward
      - name: Pull latest from main (rebase)
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
          git fetch origin main
          git pull --rebase origin main || true

      - name: Commit CSV if changed
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "chore(scraper): update scraped.csv"
          file_pattern: scraper/data/scraped.csv
          branch: main
          push_options: '--force-with-lease'
