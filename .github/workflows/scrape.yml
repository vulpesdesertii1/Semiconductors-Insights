name: Scrape news (daily)

on:
  schedule:
    - cron: "17 1 * * *"   # 01:17 UTC cada día
  workflow_dispatch:
    inputs:
      max_per_site:
        description: "Máximo de artículos por sitio"
        required: false
        default: "20"

permissions:
  contents: write

concurrency:
  group: scrape
  cancel-in-progress: true

jobs:
  scrape:
    runs-on: ubuntu-latest
    defaults:
      run:
        working-directory: scraper   # ← carpeta donde están scraper.py y requirements.txt

    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: "pip"
          cache-dependency-path: scraper/requirements.txt

      - name: Install deps
        run: pip install -r requirements.txt

      - name: Run scraper
        run: |
          MAX=${{ github.event.inputs.max_per_site || '20' }}
          python scraper.py --config sites.yaml --out data/scraped.csv --max-per-site $MAX --with-text

      - name: Commit CSV if changed
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "chore(scraper): update scraped.csv"
          file_pattern: scraper/data/scraped.csv
